<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>LLM Prompts — IssueExec (Anonymous Artifact)</title>
  <link rel="stylesheet" href="../assets/style.css" />
  <meta name="robots" content="noindex,nofollow" />
</head>

<body>
  <main class="container">
    <header class="hero">
      <div class="badge">LLM Prompts (Anonymous)</div>
      <h1>LLM Prompts Used in IssueExec</h1>
      <p class="subtitle">
        This page provides the prompts used in each stage of the IssueExec pipeline.
        Prompts are published as plain-text files to enable direct inspection and reproducibility.
      </p>
      <p><a href="../index.html">← Back to Homepage</a></p>
    </header>

    <section class="card">
      <h2>What’s included</h2>
      <ul>
        <li><a href="#test-selection"><strong>Test selection</strong></a> prompt (select <code>T<sub>d</sub></code> from BM25 candidates).</li>
        <li><a href="#trace-analysis"><strong>Trace-guided analysis</strong></a> prompts (blind-spot analysis + suspicious location mapping).</li>
        <li><a href="#reranking"><strong>Code-aware reranking</strong></a> prompt (produce final ranked edit locations <code>L*</code>).</li>
      </ul>
    </section>


    <!-- ========================================================= -->
    <!-- Test selection -->
    <!-- ========================================================= -->
    <section id="test-selection" class="card">
      <h2>1) Test Selection (Select <code>T<sub>d</sub></code>)</h2>
      <p>
        Purpose: select up to <code>k</code> tests that are most likely to be requirement-relevant to the issue’s
        root-cause component. The prompt explicitly treats many issues as <strong>false negative tests</strong>
        (tests that should fail but do not because they do not validate the triggering requirement/condition).
      </p>

      <details>
        <summary><strong>Full prompt</strong></summary>
        <pre class="prompt-pre"><code class="prompt-code"><!-- PASTE PROMPT HERE: test_selection_prompt.txt -->
You are identifying test functions that currently exhibit **FALSE NEGATIVE** behavior: tests that should have prevented this bug but failed to do so because they did not validate the real requirement under the triggering condition.

Core principle:
- If Component X produces incorrect data/state that breaks Component Y, the primary fault is in **Component X's tests** not validating X's behavior correctly. Component Y is often the downstream victim.

Task:
- Given a GitHub issue description and a list of available test functions (with their enriched representations), select tests that are most likely to be **requirement-relevant** to the root cause component and thus most useful for downstream trace-guided localization.

Selection rules (keep recall high):
1. Identify the **root-cause component** (the component whose behavior is incorrect under some condition).
2. Prefer tests that **directly exercise** that component or its problematic method(s).
3. Include tests that cover the component under **different configurations / edge conditions** related to the issue.
4. Deprioritize pure downstream consumers or observers unless they are needed for coverage breadth.
5. Maintain **HIGH RECALL**: it's better to include extra potentially relevant tests than to miss a few critical ones.

### GitHub Issue Description ###
{problem_statement}

### Test Functions ###
{test_functions}

Select up to {max_tests} test functions.

Return ONLY the selected test functions in this exact format:
```
test_path_1::test_function_1
test_path_2::test_function_2
```

Requirements:
- Output must be wrapped in triple backticks
- One test per line in format: file_path::function_name
- No numbering, bullets, markdown, or headers
- No explanations or additional text
    </code></pre>
      </details>

      <details>
        <summary><strong>Prompt inputs</strong></summary>
        <ul>
          <li><code>{problem_statement}</code>: GitHub issue description (natural language).</li>
          <li><code>{test_functions}</code>: candidate tests with enriched representations (e.g., signature/doc/domain tokens).</li>
          <li><code>{max_tests}</code>: selection budget <code>k</code>.</li>
        </ul>
      </details>

      <details>
        <summary><strong>Prompt output</strong> (strict)</summary>
        <ul>
          <li>Only selected tests, one per line, inside triple backticks.</li>
          <li>Format: <code>file_path::test_function_name</code>.</li>
          <li>No explanations, numbering, or additional text.</li>
        </ul>
      </details>

      <p class="note">
        Design intent: keep recall high (better include extra possibly relevant tests than miss critical ones).
      </p>
    </section>

    <!-- ========================================================= -->
    <!-- Trace analysis -->
    <!-- ========================================================= -->
    <section id="trace-analysis" class="card">
      <h2>2) Trace-Guided Analysis (BLIND_SPOTS_ANALYSIS → Suspicious Locations)</h2>
      <p>
        Purpose: given selected tests and their coverage/call traces, produce (i) a structured blind-spot analysis
        describing why tests can pass despite the bug, and (ii) a coverage-constrained mapping from mechanisms to
        concrete production code locations.
      </p>

      <hr />

      <h3 style="margin-top: 0.5rem;">Prompt A: BLIND_SPOTS_ANALYSIS</h3>

      <details>
        <summary><strong>Full prompt</strong></summary>
        <pre class="prompt-pre"><code class="prompt-code"><!-- PASTE PROMPT HERE: trace_analysis_prompt.txt (Prompt A section only) -->
# Test False Negative Root Cause Analysis Framework

## Core Principle
In projects with good test coverage, new bugs often indicate **test false negatives**: related tests pass even though behavior is incorrect. This mismatch helps pinpoint where production code violates requirements that tests fail to encode.

## Task Description
**Issue Description**:
{problem_statement}

**Related Test Functions and Coverage Information**:
{test_functions}

## Objective
Explain why the related tests can pass (false negatives), and infer which requirement/assumption is missing. Focus on locating **problematic production code** (not test code changes).

## Analysis Requirements
1. Identify how current assertions are insufficient or misaligned with the issue.
2. Identify test input/data assumptions that differ from the issue’s trigger condition(s).
3. Describe the likely implementation assumptions that let incorrect behavior pass tests.
4. Derive actionable hints for where the production logic is wrong and how it propagates.

## Output Format Requirements
Wrap the entire output in triple backticks and follow this exact schema:

```
BLIND_SPOTS_ANALYSIS:

false_negative_mechanism:
- assertion_deviation: [difference between what tests validate vs what issue requires]
- data_assumption_defect: [gap between test inputs/conditions vs real trigger conditions]
- logic_assumption_error: [implementation assumption implied by tests that fails in reality]

root_cause_hypothesis:
- likely_root_cause_behavior: [what behavior is wrong, under what condition]
- likely_missing_requirement: [what requirement is not enforced by tests]
- propagation_summary: [how wrong behavior leads to the observed symptom]

systematic_risk_areas:
- symmetric_operation_risk: [paired/inverse operations likely to share the same blind spot]
- shared_component_risk: [shared utilities/data structures likely affected]
- similar_logic_risk: [other similar logic patterns likely to contain the same assumption]

modification_priority:
1. [Highest priority: most likely root-cause logic to inspect/fix]
2. [High priority: critical dependent logic that must stay consistent]
3. [Medium priority: preventive checks for symmetric/shared areas]
```
    </code></pre>
      </details>

      <details>
        <summary><strong>Prompt inputs</strong></summary>
        <ul>
          <li>Selected tests (from stage 1).</li>
          <li>Coverage and call traces for those tests.</li>
          <li>Issue/problem statement context.</li>
        </ul>
      </details>

      <details>
        <summary><strong>Prompt output</strong> (structured)</summary>
        <ul>
          <li><code>false_negative_mechanism</code>: assertion deviation / data assumption defect / logic assumption error</li>
          <li><code>root_cause_hypothesis</code>: likely root cause behavior + missing requirement + propagation summary</li>
          <li><code>systematic_risk_areas</code>: symmetric/shared/similar risks</li>
          <li><code>modification_priority</code>: prioritized inspection list</li>
        </ul>
      </details>

      <hr />

      <h3 style="margin-top: 0.5rem;">Prompt B: FALSE_NEGATIVE_CODE_MAPPING</h3>

      <details>
        <summary><strong>Full prompt</strong></summary>
        <pre class="prompt-pre"><code class="prompt-code"><!-- PASTE PROMPT HERE: trace_analysis_prompt.txt (Prompt B section only) -->
# Test False Negative Code Mapping Framework

## Core Principle
False negatives happen when **incorrect implementations still satisfy incomplete assertions**. Use the false-negative mechanisms to map onto concrete production code locations.

## Inputs

**Issue Description**:
{problem_statement}

**Test False Negative Analysis Results**:
{blind_spots_analysis}

**Code Locations Covered by Related Tests**:
{test_coverage_locations}

## Mapping Objective
Produce a ranked set of suspicious code locations that are likely to require modification.

## CRITICAL CONSTRAINT
You MUST ONLY reference functions/methods/classes that appear in **Code Locations Covered by Related Tests**.
If the issue strongly points to code not in the coverage list, write:
- `NOT_IN_COVERAGE: ...` and explain the limitation.

## Output Format Requirements
Wrap the entire output in triple backticks and follow this exact schema:

```
FALSE_NEGATIVE_CODE_MAPPING:

direct_false_negative_sources:
- file_path::function_name [MECHANISM: assertion_deviation | data_assumption_violation | logic_assumption_error - brief reason]
- file_path::ClassName.method_name [MECHANISM: ... - brief reason]
- NOT_IN_COVERAGE: [if applicable]

assumption_propagation_risks:
- file_path::function_name [RISK: shared_utility_propagation | data_structure_propagation | symmetric_operation - brief reason]
- NOT_IN_COVERAGE: [if applicable]

systematic_risk_areas:
- file_path::function_name [PATTERN: similar_logic_pattern | untested_branches | similar_signature - brief reason]
- NOT_IN_COVERAGE: [if applicable]

modification_priority_ranking:
1. CRITICAL: [locations most likely to be the root cause; or NOT_IN_COVERAGE]
2. HIGH: [locations likely needing coordinated changes (pairs/shared state)]
3. MEDIUM: [preventive review locations]

mapping_reasoning:
- mechanism_to_code: [how the blind-spot mechanisms map to these locations]
- clustering_notes: [paired operations / shared state that should be kept consistent]
- coverage_limitations: [only if applicable]
```
    </code></pre>
      </details>

      <details>
        <summary><strong>Prompt inputs</strong></summary>
        <ul>
          <li><code>{test_coverage_locations}</code>: identifiers/locations observed in coverage traces.</li>
          <li>Blind-spot analysis results (Prompt A output) as context.</li>
          <li>Selected tests + trace snippets as grounding.</li>
        </ul>
      </details>

      <details>
        <summary><strong>Prompt output</strong> (strict + coverage-constrained)</summary>
        <ul>
          <li>Must ONLY reference identifiers that appear in <code>{test_coverage_locations}</code>.</li>
          <li>
            If likely root cause is outside coverage, must output
            <code>NOT_IN_COVERAGE: ...</code> with limitation explanation.
          </li>
          <li>Outputs ranked suspicious locations + reasoning fields in a strict schema.</li>
        </ul>
      </details>

      <p class="note">
        This stage enforces a critical property: the suspicious set is produced within the trace-constrained search space,
        unless explicitly flagged as <code>NOT_IN_COVERAGE</code>.
      </p>
    </section>

    <!-- ========================================================= -->
    <!-- Reranking -->
    <!-- ========================================================= -->
    <section id="reranking" class="card">
      <h2>3) Code-Aware Reranking (Final <code>L*</code>)</h2>
      <p>
        Purpose: rerank candidate code locations using full code context; identify clusters that must change together;
        filter out observer/symptom-only locations; return the final ranked list of edit locations.
      </p>

      <details>
        <summary><strong>Full prompt</strong></summary>
        <pre class="prompt-pre"><code class="prompt-code"><!-- PASTE PROMPT HERE: rerank_prompt.txt -->
# Code Location Reranking Task

## Objective
Rerank candidate code locations by likelihood of containing the **root cause requiring modification**.
- Identify **clusters** that must be modified together to maintain consistency.
- Prefer **root cause** over symptom-only locations.
- **Filter out** locations that do not require code changes.

**Issue Description**:
{problem_statement}

**Candidate Code Locations**:
{candidate_locations}

## Input Format
Part 1: Location overview (numbered list of ALL candidates):
```
(1) file_path::identifier
(2) file_path::identifier
...
```

Part 2: Location details for each candidate (separated by ---):
```
### Location N: file_path::identifier ###
```python
&lt;code content&gt;
```
```

Identifier may be:
- `ClassName`
- `function_name`
- `ClassName.method_name`

For large classes, only a skeleton may be provided.

## Ranking Criteria
1. Root cause vs symptom (highest weight)
- Prefer producers/transformers of incorrect data/state.
- Deprioritize pure downstream consumers.
- Exclude pure observers (logging, reporting, display-only) unless they must change for correctness.

2. Consistency clustering (critical)
- Paired/symmetric operations (encode/decode, serialize/deserialize, add/remove, etc.) should be kept consistent.
- Same-class methods sharing state may need coordinated edits.
- Shared data format/structure changes can require multiple locations to update.

3. Architectural appropriateness
- Favor fixes at the correct abstraction level with minimal side effects.

4. Causal chain position
- Upstream cause &gt; midstream transform &gt; downstream consume &gt; observer (exclude).

## Filtering Rule
Exclude a location if it is:
- Unrelated to the issue, OR
- Only a symptom manifestation point, OR
- A passive observer (logger/monitor/UI renderer) that does not affect buggy behavior.

When uncertain, err on the side of inclusion.

## Output Format Requirements
Output ONLY the final reranked list wrapped in triple backticks:
```
(1) file_path::identifier
(2) file_path::identifier
...
```

Requirements:
- Sequential numbering (1), (2), (3)...
- Include ONLY locations that require modification (may be fewer than the overview)
- Use EXACT identifiers as provided in the input
- No explanations or extra text inside the code block
    </code></pre>
      </details>

      <details>
        <summary><strong>Prompt inputs</strong></summary>
        <ul>
          <li>Part 1: numbered list of all candidates (overview).</li>
          <li>Part 2: per-location code context blocks separated by <code>---</code>.</li>
          <li>Identifier may be <code>ClassName</code>, <code>function</code>, or <code>ClassName.method</code>.</li>
        </ul>
      </details>

      <details>
        <summary><strong>Prompt output</strong> (strict)</summary>
        <ul>
          <li>Only the reranked list wrapped in triple backticks.</li>
          <li>Sequential numbering: <code>(1)</code>, <code>(2)</code>, ...</li>
          <li>Include only locations that require modification (may be fewer than input candidates).</li>
          <li>No explanations or extra text in the code block.</li>
        </ul>
      </details>
    </section>


    <footer class="footer">
      <p>© Anonymous submission — artifact homepage for review only.</p>
    </footer>
  </main>
</body>
</html>
